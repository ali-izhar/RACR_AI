# config/modelsplit_template.yaml

# Update this template with the appropriate configurations for your model split inference experiment

# DEFAULT CONFIGURATIONS
# ----------------------
default:
  device: cuda                  # Computation device (cuda for GPU, cpu for CPU)
  font_path: null               # Path to the font file used in visualizations; null for default
  log_level: INFO               # Default log level (INFO, DEBUG, ERROR)
  log_file: logs/app.log        # Default log file path

# MODEL CONFIGURATIONS
# --------------------
model:
  model_name: <custom_model_name> # Name of the custom model
  version: null                   # Version of the model (if applicable)
  pretrained: false               # Whether to use pretrained weights (true or false)
  weight_path: null               # Path to custom weights file (null for no custom weights)
  input_size: [c, h, w]           # Input tensor size [channels, height, width]
  hook_style: pre                 # Hook insertion style ('pre' or 'post')
  split_layer: <split_layer>      # Layer to split the model at
  save_layers: [l1, l2, l3]       # Layer indices to save intermediate outputs
  total_layers: <total_layers>    # Total number of layers in the model
  num_classes: null               # Number of classes for classification tasks (e.g., 1000 for ImageNet)
  mode: eval                      # Model operation mode ('train' for training, 'eval' for evaluation)
  depth: <depth>                  # Depth for recursive model exploration
  flush_buffer_size: <flush_buffer_size> # Number of inferences before flushing results to storage
  warmup_iterations: <warmup_iterations> # Number of warmup iterations to stabilize model performance
  log_file: logs/<custom_model>.log      # Log file for the model

# DATASET CONFIGURATIONS
# ----------------------
dataset:
  module: <custom_module>         # Python module name for the dataset (without .py extension)
  class: <CustomDataset>          # Class name of the dataset (e.g., CustomDataset)
  class_names: null               # Class names for the dataset or path to txt file containing class names
  args:
    root: <root>                  # Root directory of the dataset that contains the images
    transform: null               # Data transformations (null for default transformations)
    max_samples: -1               # Maximum number of samples to load (-1 for all samples)

# DATALOADER CONFIGURATIONS
# -------------------------
dataloader:
  batch_size: <batch_size>          # Number of samples per batch
  shuffle: false                    # Whether to shuffle the data
  num_workers: <num_workers>        # Number of subprocesses for data loading (adjust based on CPU cores)
  collate_fn: null                  # Custom collate function (set dynamically in run_dataset.py)
